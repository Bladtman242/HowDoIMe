---
title: Slider scaling
---

This is a (very, very, very delayed) response to [Baymard
Institute's](http://baymard.com/) article on [slider
inputs](http://baymard.com/blog/slider-interfaces). The article goes far beyond
what will be considered here --- and you should read it --- but in summary,
linear slider scales provide a poor user experience, because the underlying data
is rarely a uniform distribution. This creates two problems; large portions of the
slider may
<!-- you need to introduce the idea of change in the data the filtering
relationship between slider and data -->
represent no change in the underlying data, and small portions may represent
huge changes in the data. The article suggests solving these problems with the
use of biased, logarithmic, or exponential scales.

Real-world data often fit nicely on either a logarithmic or exponential curve,
but not always. I propose a method for creating biased scales --- a custom
mapping from slider-positions to values in the underlying data. In fact, I expect
this solution to fit _any_ dataset you might come up with.
<!-- based on an idea from... -->

<script id="mustache-templ" type="x-tmpl-mustache">
	<p>Filter value: {{maxvalue}}
	<br>
	Matching lenses: {{count}} of {{totaldata}}</p>
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mustache.js/2.1.3/mustache.min.js"></script>
<script src="data.js"></script>
<script>
function getPoints(slider) {
	//placeholders
	points={length:0};
	var view = {
		maxvalue: 0,
		count: 0,
		totaldata: 0,
	};

	if(slider == "slider-linear") {
		var max = document.getElementById(slider).value;
		data.forEach(function(p){
			if (p.focal_length <= max){
				points.length++;
			}
		});
		view.maxvalue = max;
		view.count = points.length;
		view.totaldata = data.length;
	}

	if(slider == "slider-equalized") {
		var sliderval = document.getElementById(slider).value;
		var max=45;
		for(i=45; i <= sliderval; i++){
				if(i in map){
					max = map[i];
				}
		}
		data.forEach(function(p){
			if (p.focal_length <= max){
				points.length++;
			}
		});
		view.maxvalue = max;
		view.count = points.length;
		view.totaldata = data.length;
	}

	return view;
}

function sliderUpdate(slider){
	var view = getPoints(slider);
	var template = document.getElementById("mustache-templ").innerHTML;
	var target = document.getElementById(slider + "-contentpane");
	var rendered = Mustache.render(template, view);
	target.innerHTML = rendered;
}

function init() {
	var setMinMax = function(slider, min, max) {
		var sliderinput = document.getElementById(slider);
		var mindiv = document.getElementById(slider + "-min");
		var maxdiv = document.getElementById(slider + "-max");

		sliderinput.min = min;
		sliderinput.max = max;
		mindiv.innerHTML = min;
		maxdiv.innerHTML = max;
	}

	//linear
	lin_max = data[data.length -1].focal_length;
	lin_min = data[0].focal_length;
	setMinMax("slider-linear", lin_min, lin_max);

	//hist equalized
	eq_max = lin_max;
	eq_min = lin_min;
	setMinMax("slider-equalized", eq_min, eq_max);

	sliderUpdate("slider-linear");
	sliderUpdate("slider-equalized");
}

</script>

##Lenshawk lenses by focal length
Before I go further in-depth with the solution, let's see an example.
The two sliders below simulate filtering on the freely available Lens Hawk
dataset, referenced in the Baymard article. For simplicity, instead of showing
all the lenses that match the filter, the number of matches is displayed.

According to the article, Lens hawk uses an exponential scale for filtering on
focal length. You can try it out [here](http://lenshawk.com/).  
(The
focal-length scale differs somehow. Their scale is 4-800, mine is 45-8000. For
reference, I took the focal_length_low_floatinger column from their database.)

---

###Linear slider

<div id="slider-linear-contentpane"></div>
<span id="slider-linear-min"></span>
<input id="slider-linear" type="range" style="width:33%;" oninput="sliderUpdate(event.target.id)">
<span id="slider-linear-max"></span>

---

###Biased slider

<div id="slider-equalized-contentpane"></div>
<span id="slider-equalized-min"></span>
<input id="slider-equalized" type="range" style="width:33%" oninput="sliderUpdate(event.target.id)">
<span id="slider-equalized-max"></span>

<script>document.onload=init()</script>


---
<br>
We can immediately see that, at the middle of the slider, the linear scale
matches $548/561$ lenses ($98\%$), where the biased scale matches $269/561$
lenses ($48\%$).

This means the right half of the linear-scale slider is used for only $2\%$ of the items,
while the remaining $98\%$ is crammed into the left half. If you play with the
sliders a little, you will notice that the linear scale puts nearly all of the lenses
within the first $\sim12\%$ of the slider, while the biased scale distributes the
lenses quite evenly.

So does the biased scale above provide a better user experience? While it seems
like a vast improvement on the linear scale, it shifts the focus. On the linear
scale, it is difficult to control the number of matching items, but on the
biased scale, it is difficult to control the exact filter value!

If the relationship between the number of matched items and the filter value can
be made apparent, the biased scale is a huge improvement. When you know of this
relationship, it is easy to see that, for a less-than-or-equal filter, the
highest value lower than the wanted max matches the exact same items as the
wanted max value. But my guess is most people would find the biased scale
unintuitive, and be annoyed that they can't pick the exact value they want. Of
course, this is just conjecture.

This is where the logarithmic or exponential
scales win; their smoothness seems to make them easier to use and understand,
despite the non-linearity.
Probably, a solution that weighs smoothness against scale utilization is possible.
<!--  values vs. slice of data-set. The user will likely have a value in mind,
and it might still be hard (or even impossible!) to find it on he biased scale
-->
<!-- possible to find a weighting between slice-distribution and value-intution?
-->


##Histogram Equalization

A common problem in image processing is contrast adjustment. Contrast adjustment
is used for example by photographers, for aesthetic purposes, and scientist have
found use for it in image segmentation, which is important used for things like
computer vision and medical image analysis (If you've ever had an MRI, CT, or X-ray,
the resulting images have likely been histogram-equalized).

Histogram equalization works for a lot of different data, but the canonical
example is monochrome images. Pixels in a monochrome image can have values
between the two extremes; black and white. But just like we didn't have lenses
with every possible focal length between 4.5 and 8000 in the Lenshawk example,
you can have images that don't utilize the space between black and white very
well. It might be that there are no or few values in the brighter end of the
spectrum (much like our Lenshawk example), or it might be that there are mostly
very dark and bright values, but not much in between.

Histogram equalization is used to assign new values to the image pixels, to
better utilize the spectrum.

There is a very direct correspondence between the problem of contrast
adjustment, and our problem of slider scaling. In contrast adjustment, we want
to maximize the use of the colour-range. In slider scaling, we want to make
optimum use of our filter values --- focal length in our case.

There is an important difference between the two problems though. In contrast
adjustment we may change the actual colours in the image, but in slider
scaling we may not adjust the values of the items we are filtering. As it turns
out, this is not a big problem --- we don't want to change the underlying
values, just how our slider maps to them!

<!-- histogram eq, image contrast analogy
show plot of distributions! (already made, in repo)
Also link to js/awk solution -->

![Distribution of lenses over focal-length] [histograms]



[histograms]: ../pics/plot.png
